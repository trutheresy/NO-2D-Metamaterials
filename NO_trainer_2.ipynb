{
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "import NO_utils\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-07-10T16:49:15.502409Z",
     "start_time": "2024-07-10T16:49:14.361108Z"
    }
   },
   "id": "ba6a3ee19aef38a3",
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load and visualize data"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "80c77908e2f33b16"
  },
  {
   "cell_type": "code",
   "id": "da5adb739d29741",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-07-10T16:49:15.503409Z"
    }
   },
   "source": [
    "# Load and extract data\n",
    "datafolder_name = [\"OUTPUT\", \"discrete_symmetric_p32_n800_rs400\"]\n",
    "data_path = os.path.join(*datafolder_name)\n",
    "print(data_path)\n",
    "\n",
    "designs, design_params, n_designs, n_panes, design_res, WAVEVECTOR_DATA, WAVEFORM_DATA, n_dim, n_wavevectors, EIGENVALUE_DATA, n_bands, EIGENVECTOR_DATA_x, EIGENVECTOR_DATA_y, const, N_struct, imag_tol, rng_seed_offset = NO_utils.extract_data(data_path)\n",
    "\n",
    "#Convert all bands to spatial domain\n",
    "bands = np.arange(1, n_bands + 1)\n",
    "bands_fft = np.zeros((n_bands, design_res, design_res))\n",
    "for i, band in enumerate(bands):\n",
    "    bands_fft[i], _ = NO_utils.const_to_spatial(band, design_res, plot_result=True, scaling_factor=1.0)\n",
    "\n",
    "#Take unique subset of waveforms    \n",
    "waveforms = WAVEFORM_DATA[0]"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OUTPUT\\discrete_symmetric_p32_n800_rs400\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Print the max and min values of all the arrays with a size of 400 in its first dimension.\n",
    "\n",
    "print(f'Max value of designs: {np.max(designs)}')\n",
    "print(f'Min value of designs: {np.min(designs)}')\n",
    "print(f'Max value of WAVEFORM_DATA: {np.max(WAVEFORM_DATA)}')\n",
    "print(f'Min value of WAVEFORM_DATA: {np.min(WAVEFORM_DATA)}')\n",
    "print(f'Max value of WAVEVECTOR_DATA_x: {np.max(EIGENVECTOR_DATA_x)}')\n",
    "print(f'Min value of WAVEVECTOR_DATA_x: {np.min(EIGENVECTOR_DATA_x)}')\n",
    "print(f'Max value of WAVEVECTOR_DATA_y: {np.max(EIGENVECTOR_DATA_y)}')\n",
    "print(f'Min value of WAVEVECTOR_DATA_y: {np.min(EIGENVECTOR_DATA_y)}')\n",
    "\n",
    "# Flatten the arrays and plot a separate histogram showing the distribution of values for each array with a size of 400 in its first dimension. Drop any imaginary components for the eigenvectors.\n",
    "\n",
    "# Flatten the arrays\n",
    "designs_flat = designs.flatten()\n",
    "WAVEFORM_DATA_flat = WAVEFORM_DATA.flatten()\n",
    "EIGENVECTOR_DATA_x_flat = EIGENVECTOR_DATA_x.flatten()\n",
    "EIGENVECTOR_DATA_y_flat = EIGENVECTOR_DATA_y.flatten()\n",
    "\n",
    "# Take the symlog of the flattened arrays. Take the real value for complex valued arrays.\n",
    "designs_flat_symlog = np.sign(designs_flat) * np.log10(np.abs(designs_flat) + 1)\n",
    "WAVEFORM_DATA_flat_symlog = np.sign(WAVEFORM_DATA_flat) * np.log10(np.abs(WAVEFORM_DATA_flat) + 1)\n",
    "EIGENVECTOR_DATA_x_flat_symlog = np.sign(EIGENVECTOR_DATA_x_flat.real) * np.log10(np.abs(EIGENVECTOR_DATA_x_flat.real) + 1)\n",
    "EIGENVECTOR_DATA_y_flat_symlog = np.sign(EIGENVECTOR_DATA_y_flat.real) * np.log10(np.abs(EIGENVECTOR_DATA_y_flat.real) + 1)"
   ],
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "is_executing": true
    }
   },
   "id": "5e7ff1c5825a1ecc",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Plot the histograms of flattened real values for each array\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 12))\n",
    "\n",
    "axes[0, 0].hist(designs_flat, bins=100, color='b', alpha=0.7)\n",
    "axes[0, 0].set_title('Designs')\n",
    "axes[0, 0].set_xlabel('Pixel Value')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "\n",
    "axes[0, 1].hist(WAVEFORM_DATA_flat, bins=100, color='r', alpha=0.7)\n",
    "axes[0, 1].set_title('Waveforms')\n",
    "axes[0, 1].set_xlabel('Value')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "\n",
    "axes[1, 0].hist(EIGENVECTOR_DATA_x_flat.real, bins=100, color='g', alpha=0.7)\n",
    "axes[1, 0].set_title('Eigenvectors (x)')\n",
    "axes[1, 0].set_xlabel('Value')\n",
    "axes[1, 0].set_ylabel('Frequency')\n",
    "\n",
    "axes[1, 1].hist(EIGENVECTOR_DATA_y_flat.real, bins=100, color='y', alpha=0.7)\n",
    "axes[1, 1].set_title('Eigenvectors (y)')\n",
    "axes[1, 1].set_xlabel('Value')\n",
    "axes[1, 1].set_ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "is_executing": true
    }
   },
   "id": "62b963bd2bde7aee",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Plot the histograms of flattened log abs values for each array\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 12))\n",
    "\n",
    "axes[0, 0].hist(designs_flat_symlog, bins=100, color='b', alpha=0.7)\n",
    "axes[0, 0].set_title('Designs')\n",
    "axes[0, 0].set_xlabel('Log Abs Value')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "\n",
    "axes[0, 1].hist(WAVEFORM_DATA_flat_symlog, bins=100, color='r', alpha=0.7)\n",
    "axes[0, 1].set_title('Waveforms')\n",
    "axes[0, 1].set_xlabel('Log Abs Value')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "\n",
    "axes[1, 0].hist(EIGENVECTOR_DATA_x_flat_symlog, bins=100, color='g', alpha=0.7)\n",
    "axes[1, 0].set_title('Eigenvectors (x)')\n",
    "axes[1, 0].set_xlabel('Log Abs Value')\n",
    "axes[1, 0].set_ylabel('Frequency')\n",
    "\n",
    "axes[1, 1].hist(EIGENVECTOR_DATA_y_flat_symlog, bins=100, color='y', alpha=0.7)\n",
    "axes[1, 1].set_title('Eigenvectors (y)')\n",
    "axes[1, 1].set_xlabel('Log Abs Value')\n",
    "axes[1, 1].set_ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "is_executing": true
    }
   },
   "id": "6d3f2966445eb4d9",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Draw a random design and its corresponding eigenvectors\n",
    "random_seed = 36\n",
    "random.seed(random_seed)\n",
    "sample_index = np.random.randint(0, n_designs)\n",
    "sample_geometry = designs[sample_index, 0]\n",
    "sample_band = np.random.randint(0, n_bands)\n",
    "sample_wavevector = np.random.randint(0, n_wavevectors)\n",
    "sample_waveform = WAVEFORM_DATA[sample_index, sample_wavevector]\n",
    "print(f'Design: {sample_index}, Band: {sample_band}, Wavevector: {sample_wavevector}')"
   ],
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "is_executing": true
    }
   },
   "id": "d14ffb26416d07b3",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Calculate the average values for geometry and waveform across all samples\n",
    "print(f'designs shape: {designs.shape}')\n",
    "print(f'WAVEFORM_DATA shape: {WAVEFORM_DATA.shape}')\n",
    "\n",
    "avg_geometry = np.mean(designs[:, 0], axis=0)\n",
    "avg_waveform = np.mean(WAVEFORM_DATA[0,:], axis=0)\n",
    "\n",
    "# Create the subplots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 12))\n",
    "\n",
    "# Plot the geometry sample on the top-left\n",
    "im1 = axes[0, 0].imshow(sample_geometry, cmap='viridis')\n",
    "axes[0, 0].set_title('Geometry Sample')\n",
    "axes[0, 0].set_xlabel('x (pixels)')\n",
    "axes[0, 0].set_ylabel('y (pixels)')\n",
    "plt.colorbar(im1, ax=axes[0, 0])\n",
    "\n",
    "# Plot the waveform sample on the top-right\n",
    "im2 = axes[0, 1].imshow(sample_waveform, cmap='viridis')\n",
    "axes[0, 1].set_title('Waveform Sample')\n",
    "axes[0, 1].set_xlabel('x (m)')\n",
    "axes[0, 1].set_ylabel('y (m)')\n",
    "plt.colorbar(im2, ax=axes[0, 1])\n",
    "\n",
    "# Plot the average geometry on the bottom-left\n",
    "im3 = axes[1, 0].imshow(avg_geometry, cmap='viridis')\n",
    "axes[1, 0].set_title('Average Geometry')\n",
    "axes[1, 0].set_xlabel('x (pixels)')\n",
    "axes[1, 0].set_ylabel('y (pixels)')\n",
    "plt.colorbar(im3, ax=axes[1, 0])\n",
    "\n",
    "# Plot the average waveform on the bottom-right\n",
    "im4 = axes[1, 1].imshow(avg_waveform, cmap='viridis')\n",
    "axes[1, 1].set_title('Average Waveform')\n",
    "axes[1, 1].set_xlabel('x (m)')\n",
    "axes[1, 1].set_ylabel('y (m)')\n",
    "plt.colorbar(im4, ax=axes[1, 1])\n",
    "\n",
    "print(f'Average pixel values of all geometries: {np.mean(designs[:,0])}')\n",
    "print(f'Average pixel values of all waveforms: {np.mean(WAVEFORM_DATA[0,:])}')"
   ],
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "is_executing": true
    }
   },
   "id": "1178eac42dcb84f2",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# View eigenvectors for the selected design, band, and wavevector\n",
    "print(f'Design: {sample_index}, Band: {sample_band}, Wavevector: {sample_wavevector}')\n",
    "sample_eigenvector_x = EIGENVECTOR_DATA_x[sample_index, sample_wavevector, sample_band]\n",
    "sample_eigenvector_y = EIGENVECTOR_DATA_y[sample_index, sample_wavevector, sample_band]\n",
    "\n",
    "NO_utils.plot_eigenvectors(sample_eigenvector_x, sample_eigenvector_y, unify_scales=True)"
   ],
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "is_executing": true
    }
   },
   "id": "initial_id",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Model architecture & training"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "403252e7c7fdba30"
  },
  {
   "cell_type": "code",
   "source": [
    "# Import modules for building a neural operator model\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.fft\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "\n",
    "# Import specialized neural operator model tools\n",
    "from neuralop.models import FNO, TFNO, FNO2d, TFNO2d\n",
    "from neuralop import Trainer\n",
    "from neuralop.training import CheckpointCallback\n",
    "from neuralop.datasets import load_darcy_flow_small\n",
    "from neuralop.utils import count_model_params\n",
    "from neuralop.datasets import data_transforms\n",
    "from neuralop import LpLoss, H1Loss\n",
    "\n",
    "# For creating datasets and data loaders for training and evaluation\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset, random_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Optional: torchvision for augmentations and transformations (if working with image-like data)\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# For plotting and visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Optional: Use tqdm for progress bars during training and evaluation\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Optional: If you are working with graph data or architectures\n",
    "# import torch_geometric\n",
    "\n",
    "# If you need automatic differentiation for custom operations or gradients\n",
    "from torch.autograd import Function"
   ],
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "is_executing": true
    }
   },
   "id": "ca8db99822c54acc",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "#Set the device for the neural operator model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print('Using Device:', device)\n",
    "print(\"Using PyTorch version:\", torch.__version__)"
   ],
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "is_executing": true
    }
   },
   "id": "f5e55a66dd87a412",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Print the shape of the first input tensor for confirmation\n",
    "print(\"Shape of the first input tensor to be assigned:\")\n",
    "print(\"designs[0, 0]:\", designs[0, 0].shape)\n",
    "print(\"WAVEFORM_DATA[0, 0]:\", WAVEFORM_DATA[0, 0].shape)\n",
    "\n",
    "# Print the shape of the first output tensor for confirmation\n",
    "print(\"Shape of the first output tensor to be assigned:\")\n",
    "print(\"EIGENVECTOR_DATA_x[0, 0, 0]:\", EIGENVECTOR_DATA_x[0, 0, 0].shape)\n",
    "print(\"EIGENVECTOR_DATA_x[0, 0, 0]:\", EIGENVECTOR_DATA_y[0, 0, 0].shape)"
   ],
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "is_executing": true
    }
   },
   "id": "e9b8e2896347d542",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "def symlog_transform(x, linthresh=1):\n",
    "    return np.sign(x) * np.log10(np.abs(x) + linthresh)\n",
    "\n",
    "def symlog_inverse_transform(y, linthresh=1):\n",
    "    return np.sign(y) * (10**np.abs(y) - linthresh)\n",
    "\n",
    "# Function to fit and transform with StandardScaler, with optional symlog transformation\n",
    "def standardize_array(array, symlog=False):\n",
    "    scaler = StandardScaler()\n",
    "    original_shape = array.shape\n",
    "    array_flat = array.reshape(-1, 1)  # Flatten the array to 2D\n",
    "\n",
    "    if symlog:\n",
    "        array_flat = symlog_transform(array_flat)\n",
    "\n",
    "    array_standardized = scaler.fit_transform(array_flat).reshape(original_shape)\n",
    "    mean = scaler.mean_[0]\n",
    "    scale = scaler.scale_[0]\n",
    "\n",
    "    return array_standardized, mean, scale, symlog"
   ],
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "is_executing": true
    }
   },
   "id": "c725e6ad5dec1c08",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Standardizing EIGENVALUE_DATA\n",
    "EIGENVALUE_DATA_standardized, eigenvalue_mean, eigenvalue_scale, _ = standardize_array(EIGENVALUE_DATA)\n",
    "\n",
    "# Standardizing EIGENVECTOR_DATA_x (real and imaginary parts separately)\n",
    "EIGENVECTOR_x_r = EIGENVECTOR_DATA_x.real\n",
    "EIGENVECTOR_x_i = EIGENVECTOR_DATA_x.imag\n",
    "\n",
    "EIGENVECTOR_x_r_standardized, eigvector_x_r_mean, eigvector_x_r_scale, _ = standardize_array(EIGENVECTOR_x_r)\n",
    "EIGENVECTOR_x_i_standardized, eigvector_x_i_mean, eigvector_x_i_scale, _ = standardize_array(EIGENVECTOR_x_i)\n",
    "\n",
    "# Standardizing EIGENVECTOR_DATA_y (real and imaginary parts separately)\n",
    "EIGENVECTOR_y_r = EIGENVECTOR_DATA_y.real\n",
    "EIGENVECTOR_y_i = EIGENVECTOR_DATA_y.imag\n",
    "\n",
    "EIGENVECTOR_y_r_standardized, eigvector_y_r_mean, eigvector_y_r_scale, _ = standardize_array(EIGENVECTOR_y_r)\n",
    "EIGENVECTOR_y_i_standardized, eigvector_y_i_mean, eigvector_y_i_scale, _ = standardize_array(EIGENVECTOR_y_i)\n",
    "\n",
    "# Standardizing WAVEVECTOR_DATA\n",
    "WAVEVECTOR_DATA_standardized, wavevector_mean, wavevector_scale, _ = standardize_array(WAVEVECTOR_DATA)"
   ],
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "is_executing": true
    }
   },
   "id": "b5942193b0603349",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Print shapes of arrays to verify\n",
    "print(\"EIGENVALUE_DATA_standardized shape:\", EIGENVALUE_DATA_standardized.shape)\n",
    "print(\"EIGENVECTOR_x_r_standardized shape:\", EIGENVECTOR_x_r_standardized.shape)\n",
    "print(\"EIGENVECTOR_x_i_standardized shape:\", EIGENVECTOR_x_i_standardized.shape)\n",
    "print(\"EIGENVECTOR_y_r_standardized shape:\", EIGENVECTOR_y_r_standardized.shape)\n",
    "print(\"EIGENVECTOR_y_i_standardized shape:\", EIGENVECTOR_y_i_standardized.shape)\n",
    "print(\"WAVEVECTOR_DATA_standardized shape:\", WAVEVECTOR_DATA_standardized.shape)\n",
    "print('\\n')\n",
    "\n",
    "# Print the mean and scale values for each standardized array\n",
    "print(\"Eigenvalue mean, scale:\", eigenvalue_mean, eigenvalue_scale)\n",
    "print(\"Eigenvector x real mean, scale:\", eigvector_x_r_mean, eigvector_x_r_scale)\n",
    "print(\"Eigenvector x imag mean, scale:\", eigvector_x_i_mean, eigvector_x_i_scale)\n",
    "print(\"Eigenvector y real mean, scale:\", eigvector_y_r_mean, eigvector_y_r_scale)\n",
    "print(\"Eigenvector y imag mean, scale:\", eigvector_y_i_mean, eigvector_y_i_scale)\n",
    "print(\"Wavevector mean, scale:\", wavevector_mean, wavevector_scale)\n",
    "print('\\n')\n",
    "\n",
    "# Print the full range of values for each standardized array\n",
    "print(\"Eigenvalue range:\", np.min(EIGENVALUE_DATA_standardized), np.max(EIGENVALUE_DATA_standardized))\n",
    "print(\"Eigenvector x real range:\", np.min(EIGENVECTOR_x_r_standardized), np.max(EIGENVECTOR_x_r_standardized))\n",
    "print(\"Eigenvector x imag range:\", np.min(EIGENVECTOR_x_i_standardized), np.max(EIGENVECTOR_x_i_standardized))\n",
    "print(\"Eigenvector y real range:\", np.min(EIGENVECTOR_y_r_standardized), np.max(EIGENVECTOR_y_r_standardized))\n",
    "print(\"Eigenvector y imag range:\", np.min(EIGENVECTOR_y_i_standardized), np.max(EIGENVECTOR_y_i_standardized))\n",
    "print(\"Wavevector range:\", np.min(WAVEVECTOR_DATA_standardized), np.max(WAVEVECTOR_DATA_standardized))"
   ],
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "is_executing": true
    }
   },
   "id": "df16d5712fc36ecd",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Standardizing EIGENVALUE_DATA\n",
    "EIGENVALUE_DATA_log_standardized, eigenvalue_log_mean, eigenvalue_log_scale, _ = standardize_array(EIGENVALUE_DATA, symlog=True)\n",
    "\n",
    "# Standardizing EIGENVECTOR_DATA_x (real and imaginary parts separately)\n",
    "# EIGENVECTOR_x_r = EIGENVECTOR_DATA_x.real\n",
    "# EIGENVECTOR_x_i = EIGENVECTOR_DATA_x.imag\n",
    "\n",
    "EIGENVECTOR_x_r_log_standardized, eigvector_x_r_log_mean, eigvector_x_r_log_scale, _ = standardize_array(EIGENVECTOR_x_r, symlog=True)\n",
    "EIGENVECTOR_x_i_log_standardized, eigvector_x_i_log_mean, eigvector_x_i_log_scale, _ = standardize_array(EIGENVECTOR_x_i, symlog=True)\n",
    "\n",
    "# Standardizing EIGENVECTOR_DATA_y (real and imaginary parts separately)\n",
    "# EIGENVECTOR_y_r = EIGENVECTOR_DATA_y.real\n",
    "# EIGENVECTOR_y_i = EIGENVECTOR_DATA_y.imag\n",
    "\n",
    "EIGENVECTOR_y_r_log_standardized, eigvector_y_r_log_mean, eigvector_y_r_log_scale, _ = standardize_array(EIGENVECTOR_y_r, symlog=True)\n",
    "EIGENVECTOR_y_i_log_standardized, eigvector_y_i_log_mean, eigvector_y_i_log_scale, _ = standardize_array(EIGENVECTOR_y_i, symlog=True)\n",
    "\n",
    "# Standardizing WAVEVECTOR_DATA\n",
    "WAVEVECTOR_DATA_log_standardized, wavevector_log_mean, wavevector_log_scale, _ = standardize_array(WAVEVECTOR_DATA, symlog=True)"
   ],
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "is_executing": true
    }
   },
   "id": "87c0a48f8828b9a9",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Print shapes of symlog arrays to verify\n",
    "print(\"EIGENVALUE_DATA_log_standardized shape:\", EIGENVALUE_DATA_log_standardized.shape)\n",
    "print(\"EIGENVECTOR_x_r_log_standardized shape:\", EIGENVECTOR_x_r_log_standardized.shape)\n",
    "print(\"EIGENVECTOR_x_i_log_standardized shape:\", EIGENVECTOR_x_i_log_standardized.shape)\n",
    "print(\"EIGENVECTOR_y_r_log_standardized shape:\", EIGENVECTOR_y_r_log_standardized.shape)\n",
    "print(\"EIGENVECTOR_y_i_log_standardized shape:\", EIGENVECTOR_y_i_log_standardized.shape)\n",
    "print(\"WAVEVECTOR_DATA_log_standardized shape:\", WAVEVECTOR_DATA_log_standardized.shape)\n",
    "print('\\n')\n",
    "\n",
    "# Print the mean and scale values for each standardized symlog array\n",
    "print(\"Eigenvalue mean, scale:\", eigenvalue_log_mean, eigenvalue_log_scale)\n",
    "print(\"Eigenvector x real mean, scale:\", eigvector_x_r_log_mean, eigvector_x_r_log_scale)\n",
    "print(\"Eigenvector x imag mean, scale:\", eigvector_x_i_log_mean, eigvector_x_i_log_scale)\n",
    "print(\"Eigenvector y real mean, scale:\", eigvector_y_r_log_mean, eigvector_y_r_log_scale)\n",
    "print(\"Eigenvector y imag mean, scale:\", eigvector_y_i_log_mean, eigvector_y_i_log_scale)\n",
    "print(\"Wavevector mean, scale:\", wavevector_log_mean, wavevector_log_scale)\n",
    "print('\\n')\n",
    "\n",
    "# Print the full range of values for each standardized symlog array\n",
    "print(\"Eigenvalue range:\", np.min(EIGENVALUE_DATA_log_standardized), np.max(EIGENVALUE_DATA_log_standardized))\n",
    "print(\"Eigenvector x real range:\", np.min(EIGENVECTOR_x_r_log_standardized), np.max(EIGENVECTOR_x_r_log_standardized))\n",
    "print(\"Eigenvector x imag range:\", np.min(EIGENVECTOR_x_i_log_standardized), np.max(EIGENVECTOR_x_i_log_standardized))\n",
    "print(\"Eigenvector y real range:\", np.min(EIGENVECTOR_y_r_log_standardized), np.max(EIGENVECTOR_y_r_log_standardized))\n",
    "print(\"Eigenvector y imag range:\", np.min(EIGENVECTOR_y_i_log_standardized), np.max(EIGENVECTOR_y_i_log_standardized))\n",
    "print(\"Wavevector range:\", np.min(WAVEVECTOR_DATA_log_standardized), np.max(WAVEVECTOR_DATA_log_standardized))"
   ],
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "is_executing": true
    }
   },
   "id": "c3ceeaecbdf8589e",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Prepare input and output data\n",
    "num_samples = n_designs * n_wavevectors * n_bands\n",
    "input_data = torch.zeros((num_samples, 3, design_res, design_res))\n",
    "output_data = torch.zeros((num_samples, 4, design_res, design_res))\n",
    "\n",
    "sample_idx = 0\n",
    "for i in range(n_designs):\n",
    "    for j in range(n_wavevectors):\n",
    "        for k in range(n_bands):\n",
    "\n",
    "            input_data[sample_idx, 0] = torch.from_numpy(designs[i, 0])\n",
    "            input_data[sample_idx, 1] = torch.from_numpy(waveforms[j])\n",
    "            input_data[sample_idx, 2] = torch.from_numpy(bands_fft[k])\n",
    "\n",
    "            output_data[sample_idx, 0] = torch.from_numpy(EIGENVECTOR_x_r_standardized[i, j, k])\n",
    "            output_data[sample_idx, 1] = torch.from_numpy(EIGENVECTOR_x_i_standardized[i, j, k])\n",
    "            output_data[sample_idx, 2] = torch.from_numpy(EIGENVECTOR_y_r_standardized[i, j, k])\n",
    "            output_data[sample_idx, 3] = torch.from_numpy(EIGENVECTOR_y_i_standardized[i, j, k])\n",
    "\n",
    "            sample_idx += 1\n",
    "\n",
    "# Create dataset\n",
    "dataset = TensorDataset(input_data, output_data)\n",
    "\n",
    "# Split dataset into train, test, and validation sets\n",
    "train_size = int(0.75 * len(dataset))\n",
    "test_size = int(0.2 * len(dataset))\n",
    "val_size = len(dataset) - train_size - test_size\n",
    "\n",
    "train_dataset, test_dataset, val_dataset = torch.utils.data.random_split(dataset, [train_size, test_size, val_size])\n",
    "\n",
    "print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "print(f\"Test dataset size: {len(test_dataset)}\")\n",
    "print(f\"Validation dataset size: {len(val_dataset)}\")"
   ],
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "is_executing": true
    }
   },
   "id": "bbc91431636d74d1",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Visualize a random sample input from the dataset\n",
    "random_idx = np.random.randint(0, len(dataset))\n",
    "random_input, random_output = dataset[random_idx]\n",
    "input_shape = random_input.shape\n",
    "\n",
    "fig, axes = plt.subplots(1, input_shape[0], figsize=(8, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i in range(3):\n",
    "    if i == 0:\n",
    "        img = random_input[i].numpy()\n",
    "    else:\n",
    "        img = random_input[i].abs().numpy()\n",
    "    axes[i].imshow(img, cmap='viridis')\n",
    "    axes[i].set_title(f'Pane {i+1}')\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "is_executing": true
    }
   },
   "id": "e25a99a85e750851",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Visualize a random sample output from the dataset\n",
    "output_shape = random_output.shape\n",
    "\n",
    "fig, axes = plt.subplots(1, output_shape[0], figsize=(8, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i in range(4):\n",
    "    if i % 2 == 0:\n",
    "        img = random_output[i].numpy()\n",
    "    else:\n",
    "        img = random_output[i].abs().numpy()\n",
    "    axes[i].imshow(img, cmap='viridis')\n",
    "    axes[i].set_title(f'Component {i+1}')\n",
    "    axes[i].axis('off')\n",
    "    \n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "is_executing": true
    }
   },
   "id": "6efb56ee142426f",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "train_dataset, test_dataset, val_dataset = random_split(dataset, [train_size, test_size, val_size])\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 256\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n"
   ],
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "is_executing": true
    }
   },
   "id": "f7a4e338379b5fbe",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Print the shapes of the input and output data, train, test, and validation sets\n",
    "print('Input data shape:', input_data.shape)\n",
    "print('Output data shape:', output_data.shape)\n",
    "print('Train data size:', len(train_dataset))\n",
    "print('Test data size:', len(test_dataset))\n",
    "print('Validation data size:', len(val_dataset))\n",
    "\n",
    "# Inspect the first batch of data\n",
    "for sample in train_loader:\n",
    "    print('Batch shape:', sample[0].shape, sample[1].shape)\n",
    "    #print(sample)\n",
    "    break"
   ],
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "is_executing": true
    }
   },
   "id": "5c9f7a57e356b6fb",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "class FourierNeuralOperator(nn.Module):\n",
    "    def __init__(self, modes_height, modes_width, in_channels=3, out_channels=4, hidden=32):\n",
    "        super(FourierNeuralOperator, self).__init__()\n",
    "        self.modes_height = modes_height  # Number of Fourier modes in height dimension\n",
    "        self.modes_width = modes_width  # Number of Fourier modes in width dimension\n",
    "        self.hidden = hidden  # Width of the hidden channels\n",
    "        self.in_channels = in_channels  # Number of input channels\n",
    "        self.out_channels = out_channels  # Number of output channels\n",
    "\n",
    "        # FNO2d layer\n",
    "        self.fno = FNO2d(in_channels=self.in_channels, out_channels=self.out_channels, n_modes_height=self.modes_height, n_modes_width=self.modes_width, hidden_channels=self.hidden, num_layers=4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Apply the FNO2d layer\n",
    "        x = self.fno(x)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "is_executing": true
    }
   },
   "id": "593f352ff2ab7c5b",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "def train_one_epoch(model, loader, criterion, optimizer, device, epoch, report_freq=100, save_after_batch=False, save_path=None):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    total_batches = len(loader)\n",
    "    progress_bar = tqdm(enumerate(loader), total=total_batches, desc=f\"Epoch {epoch + 1}\")\n",
    "\n",
    "    for batch_idx, (inputs, targets) in progress_bar:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets).mean()  # Ensure the loss is a scalar\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "        # Print progress for each batch\n",
    "        if (batch_idx + 1) % report_freq == 0 or (batch_idx + 1) == total_batches:\n",
    "            progress_bar.set_postfix(batch_loss=loss.item())\n",
    "\n",
    "        # Save model weights after each batch if the flag is set\n",
    "        if save_after_batch and save_path:\n",
    "            save_model(model, save_path)\n",
    "\n",
    "    epoch_loss = running_loss / len(loader.dataset)\n",
    "\n",
    "    # Save model weights after each epoch if the flag is not set\n",
    "    if not save_after_batch and save_path:\n",
    "        save_model(model, save_path)\n",
    "\n",
    "    return epoch_loss\n",
    "\n",
    "def evaluate(model, loader, criterion, device):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in tqdm(loader, desc=\"Evaluating\"):\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets).mean()  # Ensure the loss is a scalar\n",
    "            running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "    epoch_loss = running_loss / len(loader.dataset)\n",
    "    return epoch_loss\n",
    "\n",
    "# Function to save the model weights\n",
    "def save_model(model, file_path):\n",
    "    torch.save(model.state_dict(), file_path)\n",
    "\n",
    "# Function to load the model weights\n",
    "def load_model(model, file_path):\n",
    "    model.load_state_dict(torch.load(file_path))"
   ],
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "is_executing": true
    }
   },
   "id": "99b6262f00c1eee2",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "def plot_sample(data, title, labels, shared_colorbar=False, layout=(2, 2)):\n",
    "    fig, axes = plt.subplots(*layout, figsize=(10, 10))\n",
    "\n",
    "    if shared_colorbar:\n",
    "        # Compute global min and max for the shared colorbar\n",
    "        vmin = np.min(data.cpu().numpy())\n",
    "        vmax = np.max(data.cpu().numpy())\n",
    "\n",
    "    for ax, (idx, label) in zip(axes.flatten(), labels):\n",
    "        im = ax.imshow(data[idx, :, :].cpu(), cmap='viridis', vmin=vmin if shared_colorbar else None, vmax=vmax if shared_colorbar else None)\n",
    "        ax.set_title(label)\n",
    "        if not shared_colorbar:\n",
    "            fig.colorbar(im, ax=ax)\n",
    "\n",
    "    if shared_colorbar:\n",
    "        # Add a single colorbar to the right of the plot\n",
    "        cbar = fig.colorbar(im, ax=axes.ravel().tolist(), shrink=0.95)\n",
    "        cbar.ax.set_ylabel('Color scale')\n",
    "\n",
    "    plt.suptitle(title)\n",
    "    plt.show()\n",
    "\n",
    "def plot_inputs(data, title, labels, shared_colorbar=False):\n",
    "    plot_sample(data, title, labels, shared_colorbar, layout=(1, 3))\n",
    "\n",
    "def plot_predictions_and_targets(inputs, outputs, targets):\n",
    "    input_labels = [(0, 'geometry'), (1, 'waveform'), (2, 'band')]\n",
    "    plot_sample(inputs, 'Inputs', input_labels, shared_colorbar=True, layout=(1, 3))\n",
    "\n",
    "    prediction_labels = [(0, 'eigenvector_x_real'), (1, 'eigenvector_x_imag'), (2, 'eigenvector_y_real'), (3, 'eigenvector_y_imag')]\n",
    "    plot_sample(outputs, \"Model Predictions\", prediction_labels, shared_colorbar=True)\n",
    "\n",
    "    target_labels = [(0, 'eigenvector_x_real'), (1, 'eigenvector_x_imag'), (2, 'eigenvector_y_real'), (3, 'eigenvector_y_imag')]\n",
    "    plot_sample(targets, \"Target Values\", target_labels, shared_colorbar=True)"
   ],
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "is_executing": true
    }
   },
   "id": "fcb991698cce10d5",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "model = FourierNeuralOperator(modes_height=design_res, modes_width=design_res, in_channels=3, out_channels=4, hidden=32).to(device)\n",
    "\n",
    "# Define the loss function\n",
    "criterion = nn.MSELoss()  # Mean Squared Error loss\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=0.01)\n",
    "\n",
    "# Define the learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.1)"
   ],
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "is_executing": true
    }
   },
   "id": "1e74de86533d94ff",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Parameters for saving/loading model weights\n",
    "# save_path = './model_weights.pth'\n",
    "save_path = './model_weights_e100.pth'\n",
    "\n",
    "load_existing_model = True  # Set to True if you want to load an existing model\n",
    "\n",
    "# Load the model weights if a valid file path is provided and the flag is set\n",
    "if load_existing_model and os.path.isfile(save_path):\n",
    "    load_model(model, save_path)\n",
    "    print(f\"Loaded model weights from {save_path}\")\n",
    "\n",
    "# Lists to store losses for plotting\n",
    "train_losses = []\n",
    "val_losses = []"
   ],
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "is_executing": true
    }
   },
   "id": "2fe5e8bed0fcfb8b",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Training loop\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train_one_epoch(model, train_loader, criterion, optimizer, device, epoch, report_freq=128, save_after_batch=False, save_path=save_path)\n",
    "    \n",
    "    val_loss = evaluate(model, val_loader, criterion, device)\n",
    "\n",
    "    # Print progress for each epoch\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Training Loss: {train_loss:.4f}, Validation Loss: {val_loss:.8f}\")\n",
    "\n",
    "    # Save losses for plotting\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "\n",
    "    # Step the scheduler\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "# Plot the losses\n",
    "plt.figure()\n",
    "plt.plot(range(1, len(train_losses) + 1), train_losses, label='Training Loss')\n",
    "plt.plot(range(1, len(val_losses) + 1), val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss over Epochs')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.yscale('log')  # Set the y-axis to log scale\n",
    "plt.savefig('loss_plot.png')  # Save the plot as a PNG file\n",
    "plt.show()  # Display the plot"
   ],
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "is_executing": true
    }
   },
   "id": "187c632e4ec0f2ce",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Save the model weights at the end of training\n",
    "# save_model(model, save_path)\n",
    "save_path_posttraining = f'./model_weights_e{len(train_losses)}.pth'\n",
    "save_model(model, save_path_posttraining)\n",
    "print(f\"Saved model weights to {save_path_posttraining}\")"
   ],
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "is_executing": true
    }
   },
   "id": "f62dd41019c7b158",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Load older model weights\n",
    "# load_path = './model_weights_e101.pth'\n",
    "# load_model(model, load_path)"
   ],
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "is_executing": true
    }
   },
   "id": "d89299ed9d683809",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [
    "# Get a batch from the validation loader\n",
    "inputs, targets = next(iter(val_loader))\n",
    "inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "# Get a single sample from the batch\n",
    "input_sample = inputs[24]\n",
    "target_sample = targets[24]\n",
    "\n",
    "# Run the model to get predictions\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    output_sample = model(input_sample.unsqueeze(0)).squeeze(0)\n",
    "\n",
    "# Plot the inputs, predictions, and targets\n",
    "plot_predictions_and_targets(input_sample, output_sample, target_sample)\n"
   ],
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "is_executing": true
    }
   },
   "id": "cd1231284e89a2bd",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "is_executing": true
    }
   },
   "id": "6444107e946f4e73",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
